{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel programming with MPI for Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapted from https://github.com/rabernat/research_computing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel Computing Overview\n",
    "\n",
    "We will start the tutorial with a brief overview on parallel computing concepts:\n",
    "\n",
    " [Overview of Parallel Computing](https://www.dropbox.com/s/2yidkm4e94p0yyj/MPI%20Overview.pdf?dl=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation of mpi4py\n",
    "\n",
    "~~~\n",
    "conda install mpi4py\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is mpi4py?\n",
    "\n",
    "MPI for Python provides MPI bindings for the Python  language, allowing programmers to exploit multiple processor computing systems. mpi4py is  is constructed on top of the MPI-1/2 specifications and provides an object oriented interface which closely follows MPI-2 C++ bindings.\n",
    "\n",
    "## Documentation for mpi4py\n",
    "\n",
    "The documentation for mpi4py can be found here:\n",
    "[https://mpi4py.readthedocs.io](https://mpi4py.readthedocs.io)\n",
    "\n",
    "However, much of it assumes you are already  familiar with the MPI standard. Therefore, you will  probably also need to  consult the MPI standard documentation:\n",
    "\n",
    "[http://mpi-forum.org/docs/](http://mpi-forum.org/docs/)\n",
    "\n",
    " The MPI docs only cover the C and Fortran implementations, but the extension to Python syntax is straightforward and in most cases much simpler than the equivalent C or Fortran statements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Running Python Scripts with MPI\n",
    "\n",
    "\n",
    "Python programs that use MPI commands must be run using an MPI interpreter, which is provided with the command `mpirun`. On some systems this command is instead called `mpiexec` and mpi4py seems to include both.\n",
    "\n",
    "Make sure your environment is correct by checking that `mpirun` is in your anaconda directory for *geo_scipy* by using the `which` Unix comamnd:\n",
    "~~~\n",
    "$ which mpirun\n",
    "/anaconda/envs/geo_scipy/bin/mpirun\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run a MPI Python script using the `mpirun` command as follows:\n",
    "~~~\n",
    "mpirun -n 4 python script.py\n",
    "~~~\n",
    "Here the `-n 4` tells MPI to use four processes, which is the number of cores I have on my laptop. Then we tell MPI to run the python script named `script.py`.\n",
    "\n",
    "If you are running this on a desktop computer, then you should adjust the `-n` argument to be the number of cores on your system or the maximum number of processes needed for your job, whichever is smaller. Or on a large cluster you would specify the number of cores that your program needs or the maximum number of cores available on the particular cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Communicators and Ranks\n",
    "\n",
    "Our first MPI for python example will simply import MPI from the mpi4py package, create a *communicator* and get the *rank* of each process:\n",
    "~~~python\n",
    "from mpi4py import MPI\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "print('My rank is ',rank)\n",
    "~~~\n",
    "\n",
    "Save this to a file call `comm.py` and then run it:\n",
    "~~~python\n",
    "mpirun -n 4 python comm.py\n",
    "~~~\n",
    "Here we used the default communicator named `MPI.COMM_WORLD`, which consists of all the processors. For many MPI codes, this is the main communicator that you will need. However, you can create custom communicators using subsets of the processors in `MPI.COMM_WORLD`. See the documentation for more info."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Point-to-Point Communication\n",
    "\n",
    "Now we will look at how to pass data from one process to another. Here is a very simple example where we pass a dictionary from process 0 to process 1:\n",
    "~~~python\n",
    "# send_dict.py\n",
    "from mpi4py import MPI\n",
    "import numpy\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "if rank == 0:\n",
    "    data = {'a': 7, 'b': 3.14}\n",
    "    comm.send(data, dest=1)\n",
    "    print(f'Rank {rank} sends {data}')\n",
    "elif rank == 1:\n",
    "    data = comm.recv(source=0)\n",
    "    print(f'Rank {rank} receives {data}')\n",
    "else:\n",
    "    print(f'Rank {rank} does nothing')\n",
    "~~~    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we sent a dictionary, but you could also send an integer with a similar code:\n",
    "\n",
    "~~~python\n",
    "# send_int.py\n",
    "from mpi4py import MPI\n",
    "import numpy\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "if rank == 0:\n",
    "    idata = 2300\n",
    "    comm.send(idata, dest=1)\n",
    "    print(f'Rank {rank} sends {idata}')\n",
    "elif rank == 1:\n",
    "    data = comm.recv(source=0)\n",
    "    print(f'Rank {rank} receives {data}')\n",
    "else:\n",
    "    print(f'Rank {rank} does nothing')\n",
    "~~~\n",
    "Note how `comm.send` and `comm.recv` have lower case `s` and `r`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at a more complex example where we send a numpy array:\n",
    "\n",
    "~~~python\n",
    "# send_array.py\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "if rank == 0:\n",
    "    # in real code, this section might\n",
    "    # read in data parameters from a file\n",
    "    numData = 10  \n",
    "    comm.send(numData, dest=1)\n",
    "\n",
    "    data = np.linspace(0.0,3.14,numData)  \n",
    "    comm.Send(data, dest=1)\n",
    "\n",
    "elif rank == 1:\n",
    "\n",
    "    numData = comm.recv(source=0)\n",
    "    print('Number of data to receive: ',numData)\n",
    "\n",
    "    data = np.empty(numData, dtype='d')  # allocate space to receive the array\n",
    "    comm.Recv(data, source=0)\n",
    "\n",
    "    print('data received: ',data)\n",
    "~~~\n",
    "Note how `comm.Send` and `comm.Recv` used to send and receive the numpy array have upper case `S` and `R`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collective Communication\n",
    "\n",
    "### Broadcasting:\n",
    "Broadcasting takes a variable and sends an exact copy of it to all processes on a communicator. Here are some examples:\n",
    "\n",
    "Broadcasting a dictionary:\n",
    "~~~python\n",
    "# bcast_dict.py\n",
    "from mpi4py import MPI\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "if rank == 0:\n",
    "    data = {'key1' : [1,2, 3],\n",
    "            'key2' : ( 'abc', 'xyz')}\n",
    "else:\n",
    "    data = None\n",
    "\n",
    "data = comm.bcast(data, root=0)\n",
    "print('Rank: ',rank,', data: ' ,data)\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can broadcasting a numpy array using the `Bcast` method (again note the capital `B`). Here we will modify the point-to-point code from above to instead broadcast the array `data` to all processes in the communicator (rather than just from process 0 to 1):\n",
    "~~~python\n",
    "# bcast_array.py\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "if rank == 0:\n",
    "    # create a data array on process 0\n",
    "    # in real code, this section might\n",
    "    # read in data parameters from a file\n",
    "    numData = 10  \n",
    "    data = np.linspace(0.0,3.14,numData)  \n",
    "else:\n",
    "    numData = None\n",
    "\n",
    "# broadcast numData and allocate array on other ranks:\n",
    "numData = comm.bcast(numData, root=0)\n",
    "if rank != 0:    \n",
    "    data = np.empty(numData, dtype='d')  \n",
    "\n",
    "comm.Bcast(data, root=0) # broadcast the array from rank 0 to all others\n",
    "\n",
    "print('Rank: ',rank, ', data received: ',data)\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scattering:\n",
    "Scatter takes an array and distributes contiguous sections of it  across the ranks of a communicator. Here's an image from http://mpitutorial.com that illustrates the difference between a broadcast and scatter:\n",
    "![broadcast and scatter](http://mpitutorial.com/tutorials/mpi-scatter-gather-and-allgather/broadcastvsscatter.png \"image from http://mpitutorial.com \")\n",
    "\n",
    "Let's try an example now.\n",
    "\n",
    "~~~python\n",
    "# scatter.py\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "size = comm.Get_size() # new: gives number of ranks in comm\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "numDataPerRank = 10  \n",
    "data = None\n",
    "if rank == 0:\n",
    "    data = np.linspace(1,size*numDataPerRank,numDataPerRank*size)\n",
    "    # when size=4 (using -n 4), data = [1.0:40.0]\n",
    "\n",
    "recvbuf = np.empty(numDataPerRank, dtype='d') # allocate space for recvbuf\n",
    "comm.Scatter(data, recvbuf, root=0)\n",
    "\n",
    "print('Rank: ',rank, ', recvbuf received: ',recvbuf)\n",
    "~~~\n",
    "\n",
    "In this example, the rank 0 process created the array `data`. Since this is just a toy example, we made `data` be a simple linspace array, but in a research code the data might have been read in from a file, or generated by a previous part of the workflow.  `data` is then scattered to all the ranks (including rank 0) using `comm.Scatter`. Note that we first had to initialize (or allocate) the receiving buffer array `recvbuf`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering:\n",
    "\n",
    "The reverse of a `scatter` is a `gather`, which takes subsets of an array that are distributed across the ranks, and *gathers* them back into the full array. Here's an image from http://mpitutorial.com that illustrates this graphically:\n",
    "![broadcast and scatter](http://mpitutorial.com/tutorials/mpi-scatter-gather-and-allgather/gather.png \"image from http://mpitutorial.com \")\n",
    "\n",
    "For an example, here is a code that does the reverse of the scatter example above.\n",
    "\n",
    "~~~python\n",
    "# gather.py\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "size = comm.Get_size()\n",
    "rank = comm.Get_rank()   \n",
    "\n",
    "numDataPerRank = 10  \n",
    "sendbuf = np.linspace(rank*numDataPerRank+1,(rank+1)*numDataPerRank,numDataPerRank)\n",
    "print('Rank: ',rank, ', sendbuf: ',sendbuf)\n",
    "\n",
    "recvbuf = None\n",
    "if rank == 0:\n",
    "    recvbuf = np.empty(numDataPerRank*size, dtype='d')  \n",
    "\n",
    "comm.Gather(sendbuf, recvbuf, root=0)\n",
    "\n",
    "if rank == 0:\n",
    "    print('Rank: ',rank, ', recvbuf received: ',recvbuf)\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduction:\n",
    "\n",
    "The MP `reduce` operation takes values in from an array on each process and reduces them to a single result on the root process. This is essentially like having a somewhat complicated send command from each process to the root process, and then having the root process perform the reduction operation. Thankfully,  MPI `reduce`  does all this with one concise command.\n",
    "\n",
    " For numpy arrays, the syntax is\n",
    "\n",
    " ~~~python\n",
    "comm.Reduce(send_data, recv_data, op=<operation>, root=0)\n",
    " ~~~\n",
    " where `send_data` is the data being sent from all the processes on the communicator and `recv_data` is the array on the root process that will receive all the data. Note that the send and recv arrays have the same size.  All the copies of data in  `send_data` will be reduced according to the specified `<operation>`.  A few commonly used operations are:\n",
    "- MPI_SUM - Sums the elements.\n",
    "- MPI_PROD - Multiplies all elements.\n",
    "- MPI_MAX - Returns the maximum element.\n",
    "- MPI_MIN - Returns the minimum element.\n",
    "\n",
    "Here's a few more useful images from http://mpi_thttp://mpitutorial.com that illustrate the reduce step  graphically:\n",
    "![reduce](http://mpitutorial.com/tutorials/mpi-reduce-and-allreduce/mpi_reduce_1.png)\n",
    "![reduce](http://mpitutorial.com/tutorials/mpi-reduce-and-allreduce/mpi_reduce_2.png)\n",
    "\n",
    "And here is a code example:\n",
    "~~~python\n",
    "# reduce.py\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "# Create some np arrays on each process:\n",
    "# For this demo, the arrays have only one\n",
    "# entry that is assigned to be the rank of the processor\n",
    "value = np.array(rank,'d')\n",
    "\n",
    "print(' Rank: ',rank, ' value = ', value)\n",
    "\n",
    "# initialize the np arrays that will store the results:\n",
    "value_sum      = np.array(0.0,'d')\n",
    "value_max      = np.array(0.0,'d')\n",
    "\n",
    "# perform the reductions:\n",
    "comm.Reduce(value, value_sum, op=MPI.SUM, root=0)\n",
    "comm.Reduce(value, value_max, op=MPI.MAX, root=0)\n",
    "\n",
    "if rank == 0:\n",
    "    print(' Rank 0: value_sum =    ',value_sum)\n",
    "    print(' Rank 0: value_max =    ',value_max)\n",
    "~~~"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
